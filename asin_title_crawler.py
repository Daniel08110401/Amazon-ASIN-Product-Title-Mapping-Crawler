# ë¯¸ë§µí•‘ ASINì„ Google ê²€ìƒ‰ & Amazon ì œí’ˆ urlì—ì„œ title ì¶”ì¶œ
## ì½”ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ì‹œí‚¤ê³  'c) ì‹¤í–‰ / 1) CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ë° ê²€ì¦'ì—ì„œ input file ì£¼ì†Œ ë³€ê²½ í•„ìš”
pip install selenium
import requests
import time
import pandas as pd
import re
import json
import random
import logging, os

from bs4 import BeautifulSoup
from bs4 import BeautifulSoup
from http.client import RemoteDisconnected
from urllib.parse import urlparse, urlencode, parse_qs, quote
from selenium import webdriver
from selenium.webdriver import ChromeOptions
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

OPTIONS = ChromeOptions()

# ScrapeOps Proxy ì„¤ì •
API_KEY = ""
proxy_url = ""
# Google ìš”ì²­ í—¤ë” ì„¤ì •
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.3'
}
## a) Scapeopsë¥¼ í†µí•´ 'ASIN  amazon' ê²€ìƒ‰ 
def search_amazon_asin(asin, pages=1, retries=3, num=5):
    """
    ScrapeOpsë¥¼ ì‚¬ìš©í•˜ì—¬ Googleì—ì„œ íŠ¹ì • ASINì´ í¬í•¨ëœ Amazon ì œí’ˆ URL ê²€ìƒ‰
    """
    query = f"{asin} amazon"

    for page in range(pages):
        google_search_url = f"https://www.google.com/search?q={query}&start={page * num}&num={num}&nfpr=1"

        tries = 0
        while tries < retries:
            try:
                print(f"\nğŸ” Google ê²€ìƒ‰: {query} (í˜ì´ì§€ {page + 1})")
                response = requests.get(get_scrapeops_url(google_search_url), headers=headers)

                if response.status_code != 200:
                    print(f"âŒ ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                    raise Exception("ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜ ë°œìƒ")

                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Google ê²€ìƒ‰ ê²°ê³¼ì—ì„œ Amazon URLì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
                extracted_links = []

                for result in soup.find_all("div", class_="MjjYud"): ## yuRUbf, MjjYud class nameìœ¼ë¡œ í…ŒìŠ¤íŠ¸
                    link_tag = result.find("a", href=True, jsname=True)
                    if link_tag:
                        link = link_tag["href"]
                        extracted_links.append(link)  # ëª¨ë“  ë§í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
                        print("ğŸ”— Extracted link:", link)

                # ëª¨ë“  ë§í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ASIN ê²€ì¦
                for link in extracted_links:
                    # ASINì„ í¬í•¨í•˜ëŠ” ëª¨ë“  Amazon URL íƒìƒ‰
                    match = re.search(
                            r"amazon\.[a-z.]+(?:/-/[a-zA-Z_]+)?(?:/[^/]+)?/(?:dp|gp/product)/([A-Z0-9]{10})"
                                , link)

                    if match:
                        extracted_asin = match.group(1)  # URLì—ì„œ ì¶”ì¶œí•œ ASIN
                        print(f"ğŸ” Found ASIN in URL: {extracted_asin} (Expected: {asin})")

                        # ìš”ì²­í•œ ASINê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ë°˜í™˜
                        if extracted_asin == asin:
                            print(f"âœ… ASIN {asin} â†’ URL ë°œê²¬: {link}")
                            return link
                
                print(f"âŒ ASIN {asin} â†’ ìœ íš¨í•œ Amazon URL ì°¾ì§€ ëª»í•¨ (í˜ì´ì§€ {page + 1})")
                return None

            except Exception as e:
                print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘ ({tries + 1}/{retries})")
                time.sleep(3)
                tries += 1

    return None  # ASINì„ í¬í•¨í•œ URLì´ ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ

def check_asin_in_url(asin, asin_url_map):
    """
    íŠ¹ì • ASINì´ í¬í•¨ëœ Amazon ì œí’ˆ URLì„ Google ê²€ìƒ‰ì—ì„œ ì°¾ê³  asin_url_mapì— ì €ì¥
    param asin: Amazon ASIN ì½”ë“œ
    param asin_url_map: ASINë³„ Amazon URL ì €ì¥ ë”•ì…”ë„ˆë¦¬
    """
    amazon_url = search_amazon_asin(asin)

    if amazon_url:
        asin_url_map[asin] = amazon_url
        print(f"âœ… ASIN {asin} â†’ URL ì €ì¥ ì™„ë£Œ: {amazon_url}")
    else:
        asin_url_map[asin] = "No Data"
        print(f"âŒ ASIN {asin} â†’ Amazon URL ê²€ìƒ‰ ì‹¤íŒ¨")
## b) amazon ì œí’ˆ ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•´ì„œ asin ì½”ë“œ ê²€ì¦ & title ì¶”ì¶œ
### 1ì°¨ ì¡°íšŒ : Request & Session 
# ìš”ì²­ í—¤ë” ì„¤ì • (Amazon í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
amazon_headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
    "Referer": "https://www.google.com/",
    "Accept-Language": "en-US,en;q=0.9",
    "DNT": "1", 
    "Upgrade-Insecure-Requests": "1"
}

# ì„¸ì…˜ ìƒì„± (ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ ì°¨ë‹¨ ê°€ëŠ¥ì„± ì¤„ì´ê¸°)
session = requests.Session()
session.headers.update(amazon_headers)

def fetch_product_title(asin_url_map, product_url, asin, asin_title_map, driver):
    max_retries_amazon = 3
    retry_sleep_sequence = [3, 5, 7]

    if asin in asin_title_map:
        print(f"ğŸ” ì¤‘ë³µ ìš”ì²­ ë°©ì§€: ASIN {asin}ì€ ì´ë¯¸ ì¡°íšŒë¨.")
        return asin_title_map[asin]

    if asin_url_map.get(asin) in ["ìˆ˜ê¸° ìš”ì²­", "No Data"]:
        return asin_url_map.get(asin)

    random_delay = random.randint(5, 12)
    print(f"\n[{asin}] ìš”ì²­ ì „ ëœë¤ ëŒ€ê¸°: {random_delay}ì´ˆ")
    time.sleep(random_delay)

    for attempt in range(1, max_retries_amazon + 1):
        try:
            print(f"\nğŸ” [{attempt}/{max_retries_amazon}] Amazonì— {asin} ìš”ì²­ ì¤‘: {product_url}")
            response = session.get(product_url, headers=amazon_headers, allow_redirects=True, timeout=10)

            ## ì„œë²„ ìš”ì²­ ê²°ê³¼ì— ë”°ë¥¸ í•„í„°ë§ ##
            status = response.status_code
            if status in [500, 503]:
                print(f"ğŸš¨ {status} Error ë°œìƒ, {attempt}/{max_retries_amazon}íšŒ ì¬ì‹œë„ ì¤‘...")
                time.sleep(retry_sleep_sequence[min(attempt - 1, len(retry_sleep_sequence)-1)])
                continue
            elif status == 404:
                print(f"ğŸš¨ 404 Not Found: {product_url} â†’ 'No Data' ë°˜í™˜")
                return "No Data"

            response.raise_for_status()

            ## redirection ì²˜ë¦¬ ##
            original_asin = re.search(r"/dp/([A-Z0-9]{10})", product_url).group(1)
            final_url = response.url
            final_asin = re.search(r"/dp/([A-Z0-9]{10})", final_url).group(1)
            if original_asin and final_asin:
                if original_asin != final_asin:
                    print(f"âŒ ASIN ë¶ˆì¼ì¹˜! ìš”ì²­ ASIN: {original_asin} | ë¦¬ë””ë ‰ì…˜ëœ ASIN: {final_asin}")
                    return "No Data"
            ## captcha ì²˜ë¦¬ ##
            if "captcha" in response.text.lower():
                print(f"ğŸ›‘ CAPTCHA ê°ì§€ë¨ â€” ScrapOpsë¡œ ì „í™˜")
                return fetch_title_scrapeops(product_url, asin, driver)
            
            # HTML elements ë¶ˆëŸ¬ì˜¤ê¸°
            soup = BeautifulSoup(response.text, "html.parser")

            ## ASIN ì¶”ì¶œ í›„ ëŒ€ì¡°
            # Regular expression pattern for ASIN (10 alphanumeric characters)
            # Search for ASIN using regex in the page content
            asin_pattern = r'/dp/([A-Z0-9]{10})'
            asin_match = re.search(asin_pattern, soup.prettify())
            asin_extracted = asin_match.group(1)
            print(f"ğŸ” ASIN extracted : {asin_extracted}")
            if asin_extracted != asin:
                print(f"âŒ ASIN ë¶ˆì¼ì¹˜! ìš”ì²­ ASIN: {asin} | ì¡°íšŒëœ ASIN: {asin_extracted}")
                return "No Data"
    
            ## í˜ì´ì§€ ë¡œë”© ê²°ê³¼ì— ë”°ë¥¸ í•„í„°ë§ ##
            ## ============================ ##
            # ì•„ë§ˆì¡´ "We're sorry" ì²´í¬ 
            if soup.find("img", src=re.compile("title\\._ttd_\\.png", re.IGNORECASE)):
                print("âŒ Page Not Found â€” No Data")
                return "No Data"

            title_tag = soup.find("span", id="productTitle") or soup.find("span", id="title")
            if title_tag:
                product_title = title_tag.text.strip()
                print(f"âœ… Title (HTML ì¶”ì¶œ): {product_title}")
                asin_title_map[asin] = product_title
                return product_title

            print(f"âŒ ì œí’ˆ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ScrapOpsë¡œ ì „í™˜")
            return fetch_title_scrapeops(product_url, asin, driver)

        except requests.exceptions.RequestException as e:
            print(f"ğŸš¨ Amazon ìš”ì²­ ì‹¤íŒ¨: {e}")

    print(f"â¡ï¸ ìš”ì²­ ì‹¤íŒ¨, ScrapOps ì‚¬ìš©")
    return fetch_title_scrapeops(product_url, asin, driver)

### 2ì°¨ ì¡°íšŒ : ScrapeOps Title Extraction
def init_driver(headless=True):
    options = Options()
    if headless:
        options.add_argument("--headless=new") # ë¸Œë¼ìš°ì €ë¥¼ ì—´ì§€ ì•Šë„ë¡ ì„¤ì •
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/119 Safari/537.36")
        options.add_argument("--no-sandbox") # Linux ê°™ì€ GUI í™˜ê²½ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê³³ì—ì„œ ì‚¬ìš©
        options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=options)
    return driver
def get_scrapeops_url(original_url: str) -> str:
    encoded_url = quote(original_url, safe="")
    return f"https://proxy.scrapeops.io/v1/?api_key={API_KEY}&url={encoded_url}&country=us"

def fetch_title_scrapeops(product_url: str, asin: str, driver) -> str:
    try:
        driver.get(get_scrapeops_url(product_url))
        print(f"ğŸŒ [ScrapeOps] Fetched page with Selenium for ASIN: {asin}")
        page_source = driver.page_source.lower()

        ## Internal failure ì²˜ë¦¬ ë¡œì§ ##
        if "failed to get successful response" in page_source:
            print("âŒ [ScrapOps] Internal fetch failure")
            if not retry:
                print("ğŸ” ì¬ì‹œë„ ì¤‘ (ScrapeOps)")
                return fetch_title_scrapeops(product_url, asin, driver, retry=True)
            return "ìˆ˜ê¸° ìš”ì²­"
            
        ## Page not found ì²˜ë¦¬ ë¡œì§ ##
        if ("sorry! we couldn't find that page" in page_source or
            "we couldn't find that page" in page_source or
            "title._ttd_.png" in page_source):
            print("âŒ Page Not Found â€” No Data")
            return "No Data"

        
        ## Url redirection ì²˜ë¦¬: ASINì´ ë‹¤ë¥´ë©´ ë¦¬ë””ë ‰ì…˜ ##
        asin_match = re.search(r'/dp/([a-z0-9]{10})', page_source)
        if asin_match:
            final_asin = asin_match.group(1).upper()
            if asin != final_asin:
                print(f"âŒ ASIN ë¶ˆì¼ì¹˜! ìš”ì²­ ASIN: {asin} | í˜ì´ì§€ ë‚´ ì¡°íšŒëœ ASIN: {final_asin}")
                return "No Data"
            else:
                print(f"âœ… ìš”ì²­ëœ {asin}ê³¼ í˜ì´ì§€ ë‚´ ì¡°íšŒ {final_asin} ì¼ì¹˜")
                
    except Exception as e:
        print(f"âš ï¸ [ScrapeOps] ìš”ì²­ ì‹¤íŒ¨: {e}")

    print(f"âŒ ìµœì¢… ì‹¤íŒ¨, ìˆ˜ê¸° ìš”ì²­ í•„ìš”: {product_url}")
    return "ìˆ˜ê¸° ìš”ì²­"

def update_asin_data_hybrid(input_file_path, asin_url_map, asin_title_map):
    df = pd.read_csv(input_file_path, dtype=str)

    # action_required ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    if "action_required" not in df.columns:
        df["action_required"] = ""
    if "item_name" not in df.columns:
        df["item_name"] = ""

    # 1) ë¸Œë¼ìš°ì €(ChromeDriver) ì´ˆê¸°í™”
    driver = init_driver(headless=True)  # headless=Falseë¡œ í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ

    for asin, product_url in asin_url_map.items():
        if asin in asin_title_map:
            print(f"ğŸ” {asin} ì´ë¯¸ ì²˜ë¦¬ë¨, ê±´ë„ˆëœ€")
            continue

        if product_url == "No Data":
            asin_title_map[asin] = "No Data"
            continue

        product_title = fetch_product_title(redirected_asin, product_url, asin, asin_title_map, driver)
        asin_title_map[asin] = product_title

    # 2) ê²°ê³¼ë¥¼ csv íŒŒì¼ì— ë°˜ì˜
    for asin, product_title in asin_title_map.items():
        if product_title == "ìˆ˜ê¸° ìš”ì²­":
            df.loc[df["asin"] == asin, "action_required"] = "ìˆ˜ê¸° ìš”ì²­"
            df.loc[df["asin"] == asin, "item_name"] = ""
        elif product_title == "No Data":
            df.loc[df["asin"] == asin, "action_required"] = "No Data"
            df.loc[df["asin"] == asin, "item_name"] = ""
        else:
            df.loc[df["asin"] == asin, "item_name"] = product_title
            df.loc[df["asin"] == asin, "action_required"] = ""

    updated_file_path = "asin_crawling_" + input_file_path
    df.to_csv(updated_file_path, index=False, encoding="utf-8-sig")

    print(f"\nâœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {updated_file_path}")

    # 5) ë§ˆë¬´ë¦¬: WebDriver ì¢…ë£Œ
    driver.quit()
## c) ì‹¤í–‰
### 1) CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ë° ê²€ì¦
input_file_path = "ë¯¸ì¡°íšŒ_asin_425_250224.csv" # ì¡°íšŒí•´ì•¼ í•  CSV íŒŒì¼ ê²½ë¡œ, íŒŒì¼ì— ë”°ë¼ ì…ë ¥ í•„ìš”

# CSV íŒŒì¼ ì½ê¸°
try:
    df = pd.read_csv(input_file_path, dtype=str)
    print(f"CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {input_file_path}")
except FileNotFoundError:
    print(f"ì—ëŸ¬: íŒŒì¼ '{input_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    df = None

# asin ì»¬ëŸ¼ df_asin_to_test ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
if df is not None:
    if "asin" in df.columns:
        df_asin_list = df["asin"].dropna().astype(str).tolist() # íŠ¹ì • ê°œìˆ˜ì˜ asinì„ í…ŒìŠ¤íŠ¸ í•˜ê³  ì‹¶ë‹¤ë©´ [:num] ì¶”ê°€ ex) tolist()[:10]
        print(f"ğŸ” ê²€ìƒ‰í•  ASIN ê°œìˆ˜: {len(df_asin_list)}")
    else:
        print("CSV íŒŒì¼ì— 'asin' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        df_asin_list = []
else:
    df_asin_list = [] # df_asin_to_test : ì¡°íšŒ í•  asin ì½”ë“œ ë‹´ê²¨ì ¸ ìˆëŠ” ë¦¬ìŠ¤íŠ¸

# df_asin_to_test ì¶œë ¥
print(f"í…ŒìŠ¤íŠ¸í•  ASIN ë¦¬ìŠ¤íŠ¸: {df_asin_list}")
### 1-1) ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¬ì‹œ, ì•„ë˜ ì½”ë“œ ì°¸ì¡°
# # ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
# input_file_path = "asin_400_20250224.xlsx"  # í™•ì¥ìë¥¼ .xlsxë¡œ ë³€ê²½

# # ì—‘ì…€ íŒŒì¼ ì½ê¸°
# try:
#     df = pd.read_excel(input_file_path, sheet_name="asin", dtype=str)  # 'asin' ì‹œíŠ¸ ì½ê¸°
#     print(f"ğŸ“‚ ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {input_file_path} (ì‹œíŠ¸: 'asin')")
# except FileNotFoundError:
#     print(f"âŒ ì—ëŸ¬: íŒŒì¼ '{input_file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     df = None
# except ValueError:
#     print(f"âŒ ì—ëŸ¬: íŒŒì¼ì— 'asin' ì‹œíŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#     df = None

# # ASIN ì»¬ëŸ¼ ê°’ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
# if df is not None:
#     if "asin" in df.columns:
#         df_asin_list = df["asin"].dropna().astype(str).tolist()  # ASIN ë¦¬ìŠ¤íŠ¸ ìƒì„±
#         print(f"ğŸ” ê²€ìƒ‰í•  ASIN ê°œìˆ˜: {len(df_asin_list)}")
#     else:
#         print("âŒ ì—‘ì…€ íŒŒì¼ì— 'asin' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
#         df_asin_list = []
# else:
#     df_asin_list = []  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# # ê²°ê³¼ ì¶œë ¥
# print(f"ğŸ“Œ í…ŒìŠ¤íŠ¸í•  ASIN ë¦¬ìŠ¤íŠ¸: {df_asin_list}")
### 2) ASIN êµ¬ê¸€ ê²€ìƒ‰ ì‹¤í–‰
### 3) ì œí’ˆ ì‚¬ì´íŠ¸ asin ì¡°íšŒ ë° ê²€ì¦, ê²€ì¦ ì™„ë£Œ asinì€ title ì¶”ì¶œ
asin_url_map = {}
asin_title_map = {}

## 'ASIN amazon' êµ¬ê¸€ ê²€ìƒ‰ ì‹¤í–‰ ##
#asin_url_mapì— ê²°ê³¼ ì ì¬

for asin in df_asin_list:
    check_asin_in_url(asin, asin_url_map)

## amazon urlì— ì ‘ì†, asin ì½”ë“œ ê²€ì¦ í›„ titleê°’ì„ csvíŒŒì¼ ì—…ë¡œë“œ ##
# asin_url_map ì ì¬ëœ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆ title ì¶”ì¶œ

update_asin_data_hybrid(input_file_path, asin_url_map, asin_title_map)
